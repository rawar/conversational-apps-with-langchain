{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt engineering\n",
    "\n",
    "Prompts consist of three main components: \n",
    "\n",
    "* __Instructions__ that describe the task requirements, goals, and format of input/output. They explain the task to the model unambiguously. \n",
    "* __Examples__ that demonstrate the desired input-output pairs. They provide diverse demonstrations of how different inputs should map to outputs. \n",
    "* __Input__ that the model must act on to generate the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot Prompting\n",
    "\n",
    "* No examples provided; rely on the models training\n",
    "* Leverages the models pre-training\n",
    "* Works for simple tasks, but struggles with complex reasoning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.cache import SQLiteCache\n",
    "from IPython.display import display, Markdown\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(base_url=\"http://192.168.178.84:11434\", model=\"gemma:7b\", temperature=0)\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"text\"], template=\"Classify the sentiment of this text: {text}\")\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"text\": \"I hate the movie Batman & Robin, it was terrible!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The sentiment of the text is negative. The speaker is expressing hate and negativity towards the movie Batman & Robin."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot Prompting\n",
    "\n",
    "* Provide a few demos of input and desired output\n",
    "* Shows desired reasoning format\n",
    "* Tripled accuracy on grade-school math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\"\n",
    "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus.\n",
    " \n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure, here is the revised text:\n",
       "\n",
       "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word \"whatpu\" is:\n",
       "\n",
       "We were traveling in Africa and we saw these very cute whatpus.\n",
       "\n",
       "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word \"farduddle\" is:\n",
       "\n",
       "We were hiking in the forest and we saw a group of people farddling down the hill."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_1 = llm.invoke(prompt_1)\n",
    "display(Markdown(result_1.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-Thought (CoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"\"\"\n",
    "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure, here is the solution:\n",
       "\n",
       "1. You bought 10 apples, gave 2 apples to the neighbor and 2 apples to the repairman, so you have 10 - 2 - 2 = 6 apples left.\n",
       "2. You bought 5 more apples, so you have 6 + 5 = 11 apples.\n",
       "3. You ate 1 apple, so you have 11 - 1 = 10 apples left.\n",
       "\n",
       "Therefore, you have a total of 10 apples remaining."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_2 = llm.invoke(prompt_2)\n",
    "display(Markdown(result_2.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_3 = \"\"\"\n",
    "Given this context: ```Jeff and Tommy are neighbors. Tommy and Eddy are not neighbors.``` and the following query: ```Are Jeff and Eddy neighbors?```. \n",
    "Please answer the questions in the query and explain your reasoning.\n",
    "If there is not enough informaton to answer, please say\n",
    "\"I do not have enough information to answer this questions.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Answer:** I do not have enough information to answer this question.\n",
       "\n",
       "The text provided does not mention any relationship between Jeff and Eddy, therefore I cannot answer the query."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_3 = llm.invoke(prompt_3)\n",
    "display(Markdown(result_3.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least-to-most Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-verification (CoV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-of_Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
