{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt engineering\n",
    "\n",
    "Prompts consist of three main components: \n",
    "\n",
    "* __Instructions__ that describe the task requirements, goals, and format of input/output. They explain the task to the model unambiguously. \n",
    "* __Examples__ that demonstrate the desired input-output pairs. They provide diverse demonstrations of how different inputs should map to outputs. \n",
    "* __Input__ that the model must act on to generate the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot Prompting\n",
    "\n",
    "* No examples provided; rely on the models training\n",
    "* Leverages the models pre-training\n",
    "* Works for simple tasks, but struggles with complex reasoning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.cache import SQLiteCache\n",
    "from IPython.display import display, Markdown, JSON\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import SequentialChain, LLMChain\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = ChatOllama(base_url=\"http://localhost:11434\", model=\"gemma:7b\", temperature=0.5)\n",
    "#set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"text\"], template=\"Classify the sentiment of this text: {text}\")\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"text\": \"I hate the movie Batman & Robin, it was terrible!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The sentiment of this text is negative."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot Prompting\n",
    "\n",
    "* Provide a few demos of input and desired output\n",
    "* Shows desired reasoning format\n",
    "* Tripled accuracy on grade-school math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\"\n",
    "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus.\n",
    " \n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The children began to farduddle when they heard the ice cream truck coming down the street."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_1 = llm.invoke(prompt_1)\n",
    "display(Markdown(result_1.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"\"\"\n",
    "Positive This is awesome! \n",
    "This is bad! Negative\n",
    "Wow that movie was rad!\n",
    "Positive\n",
    "What a horrible show! --\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Negative"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_2 = llm.invoke(prompt_2)\n",
    "display(Markdown(result_2.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-Thought (CoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_3 = \"\"\"\n",
    "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You remained with 10 apples."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_3 = llm.invoke(prompt_3)\n",
    "display(Markdown(result_3.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_4 = \"\"\"\n",
    "Given this context: ```Jeff and Tommy are neighbors. Tommy and Eddy are not neighbors.``` and the following query: ```Are Jeff and Eddy neighbors?```. \n",
    "Please answer the questions in the query and explain your reasoning.\n",
    "If there is not enough informaton to answer, please say\n",
    "\"I do not have enough information to answer this questions.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I do not have enough information to answer this question. The context does not provide any information about the relationship between Jeff and Eddy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_4 = llm.invoke(prompt_4)\n",
    "display(Markdown(result_4.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_5 = \"\"\"\n",
    "Roger has 5 tennis balls. \n",
    "He buys 2 more cans of tennis balls. \n",
    "Each can has 3 tennis balls. \n",
    "How many tennis balls does he have now?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Roger has 11 tennis balls now."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_5 = llm.invoke(prompt_5)\n",
    "display(Markdown(result_5.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least-to-most Prompting\n",
    "\n",
    "Least to Most prompting (LtM) takes CoT prompting a step further by first breaking a problem into sub problems then solving each one. It is a technique inspired by real-world educational strategies for children.\n",
    "\n",
    "LtM leads to multiple improvements:\n",
    "\n",
    "* improved accuracy over Chain of Thought\n",
    "* increased generalization on problems harder than those in the prompt\n",
    "* dramatically improved performance in compositional generalization, in particular the SCAN benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_6 = \"\"\"\n",
    "Q: turn left\n",
    "A: TURN LEFT\n",
    "\n",
    "Q: turn right\n",
    "A: TURN RIGHT\n",
    "\n",
    "Q: jump left\n",
    "A: TURN LEFT + JUMP\n",
    "\n",
    "Q: run right\n",
    "A: TURN RIGHT + RUN\n",
    "\n",
    "Q: look twice\n",
    "A: LOOK * 2\n",
    "\n",
    "Q: run and look twice\n",
    "A: RUN + LOOK * 2\n",
    "\n",
    "Q: jump right thrice\n",
    "A: (TURN RIGHT + JUMP) * 3\n",
    "\n",
    "Q: walk after run\n",
    "A: RUN + WALK\n",
    "\n",
    "Q: turn opposite left\n",
    "A: TURN LEFT * 2\n",
    "\n",
    "Q: turn around left\n",
    "A: TURN LEFT * 4\n",
    "\n",
    "Q: turn opposite right\n",
    "A: TURN RIGHT * 2\n",
    "\n",
    "Q: turn around right\n",
    "A: TURN RIGHT * 4\n",
    "\n",
    "Q: walk opposite left\n",
    "A: TURN LEFT * 2 + WALK\n",
    "\n",
    "Q: walk around left\n",
    "A: (TURN LEFT + WALK) * 4\n",
    "\n",
    "Q: \"jump around left twice after walk opposite left thrice\" \n",
    "A:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "(TURN LEFT * 2 + WALK) * 3 + (TURN LEFT + JUMP) * 2 * 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_6 = llm.invoke(prompt_6)\n",
    "display(Markdown(result_6.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_7 = \"\"\"\n",
    "INSTRUCTIONS:\n",
    "You are a customer service agent tasked with kindly responding to customer inquiries. \n",
    "Returns are allowed within 30 days. \n",
    "Today's date is March 29th. \n",
    "There is currently a 50% discount on all shirts. \n",
    "Shirt prices range from $18-$100 at your store. \n",
    "Do not make up any information about discount policies.\n",
    "What subproblems must be solved before answering the inquiry?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. Understanding the specific inquiry: Determine what exactly the customer is asking. Are they asking about the return policy, the current sale, or the price range of the shirts? \n",
       "\n",
       "2. Determine the purchase date: If the customer is inquiring about a return, find out when the item was purchased to ensure it falls within the 30-day return policy. \n",
       "\n",
       "3. Determine the item in question: If the customer is asking about the discount or price, ensure they are referring to shirts since the discount only applies to shirts.\n",
       "\n",
       "4. Calculate the sale price: If the customer is asking about the discounted price of a shirt, calculate the sale price based on the original price and the 50% discount. \n",
       "\n",
       "5. Understand the store's discount policies: Check if there are any exceptions or specific conditions to the discount to accurately inform the customer.\n",
       "\n",
       "6. Understand the store's return policies: Ensure you are familiar with the full return policy to accurately inform the customer, including any conditions or exceptions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_7 = llm.invoke(prompt_7)\n",
    "display(Markdown(result_7.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_8 = \"\"\"\n",
    "CUSTOMER INQUIRY:\n",
    "I just bought a T-shirt from your Arnold collection on March 1st. \n",
    "I saw that it was on discount, so bought a shirt that was originally $30, and got 40% off. \n",
    "I saw that you have a new discount for shirts at 50%. \n",
    "I'm wondering if I can return the shirt and have enough store credit to buy two of your shirts?\n",
    "\n",
    "INSTRUCTIONS:\n",
    "You are a customer service agent tasked with kindly responding to customer inquiries. \n",
    "Returns are allowed within 30 days. T\n",
    "oday's date is March 29th. There is currently a 50% discount on all shirts. \n",
    "Shirt prices range from $18-$100 at your store. \n",
    "Do not make up any information about discount policies.\n",
    "Determine if the customer is within the 30-day return window. Let's go step by step.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Dear Customer,\n",
       "\n",
       "Thank you for reaching out with your inquiry. I'd be happy to clarify this for you.\n",
       "\n",
       "Firstly, I can confirm that you are within the 30-day return window as you bought the shirt on March 1st and today's date is March 29th. \n",
       "\n",
       "You mentioned that you purchased a shirt from our Arnold collection that was originally $30 and you received a 40% discount. This means you paid $18 for the shirt ($30 - 40% = $18).\n",
       "\n",
       "Our current discount is 50% on all shirts. If you were to return your shirt and receive store credit, you would have $18 to spend. Even with the 50% discount, this would not be enough to purchase two shirts unless they were priced at $18 or less each. Most of our shirts are priced between $18-$100, so it would depend on the specific shirts you're interested in.\n",
       "\n",
       "I hope this information is helpful. If you have any other questions or need further clarification, please don't hesitate to ask.\n",
       "\n",
       "Kind Regards,\n",
       "[Your Name]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_8 = llm.invoke(prompt_8)\n",
    "display(Markdown(result_8.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-consistency\n",
    "\n",
    "Self-consistency prompting is a technique that generates multiple chain-of-thoughts by prompting the model several times to obtain different outputs. More at https://www.promptingguide.ai/techniques/consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_9 = \"\"\"\n",
    "When I was 6 my sister was half my age. Now\n",
    "I’m 70 how old is my sister?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "She would be 67 years old."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# should be 67\n",
    "result_9 = llm.invoke(prompt_9)\n",
    "display(Markdown(result_9.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_10 = \"\"\"\n",
    "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
    "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
    "did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
    "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
    "he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
    "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
    "monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
    "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
    "The answer is 29.\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
    "golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
    "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: She bought 5 bagels for $3 each. This means she spent $15. She has $8 left.\n",
    "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
    "A:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When you were 6, your sister was half your age, which means she was 3 years old. So, your sister is 3 years younger than you. If you're now 70, your sister is 70 - 3 = 67 years old. The answer is 67."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_10 = llm.invoke(prompt_10)\n",
    "display(Markdown(result_10.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-density (CoD)\n",
    "\n",
    "How Chain of Density Prompting Works\n",
    "\n",
    "1. Initial Assessment: The AI first assesses the original text to determine its overall complexity, length, and structure. This initial assessment forms the basis for how the summarization process will proceed.\n",
    "1. Determining Baseline Density: Based on the initial assessment, the AI sets a baseline density level that corresponds to the expected level of detail and conciseness for the summary. This baseline acts as a starting point for further refinement.\n",
    "1. Progressive Refinement: The AI then goes through a series of density adjustments, each time refining the level of detail based on feedback loops and predefined quality metrics. This step is where the \"chain\" aspect comes into play, with each link in the chain representing a stage of refinement.\n",
    "1. Final Summarization: Once the optimal density level is achieved, the AI generates the final summary, ensuring that it is coherent, concise, and maintains the essence of the original text.\n",
    "\n",
    "More details https://advanced-stack.com/resources/how-to-summarize-using-chain-of-density-prompting.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "What is a Large Language Model (LLM)?\n",
    "\n",
    "A large language model is an advanced type of language model that is trained using deep learning techniques on massive amounts of text data. \n",
    "These models are capable of generating human-like text and performing various natural language processing tasks.\n",
    "\n",
    "In contrast, the definition of a language model refers to the concept of assigning probabilities to sequences of words, based on the analysis of text corpora. \n",
    "A language model can be of varying complexity, from simple n-gram models to more sophisticated neural network models. \n",
    "However, the term “large language model” usually refers to models that use deep learning techniques and have a large number of parameters, which can range from millions to billions. \n",
    "These models can capture complex patterns in language and produce text that is often indistinguishable from that written by humans.\n",
    "\n",
    "How a Large Language Model (LLM) Is Built?\n",
    "\n",
    "A large-scale transformer model known as a “large language model” is typically too massive to run on a single computer and is, therefore, provided as a service over an API or web interface. \n",
    "These models are trained on vast amounts of text data from sources such as books, articles, websites, and numerous other forms of written content. \n",
    "By analyzing the statistical relationships between words, phrases, and sentences through this training process, the models can generate coherent and contextually relevant responses to prompts or queries.\n",
    "ChatGPT’s GPT-4, a large language model, was trained on massive amounts of internet text data, allowing it to understand various languages and possess knowledge of diverse topics. \n",
    "As a result, it can produce text in multiple styles. \n",
    "While its capabilities, including translation, text summarization, and question-answering, may seem impressive, they are not surprising, given that these functions operate using special “grammars” that match up with prompts.\n",
    "\n",
    "How do large language models work?\n",
    "\n",
    "Large language models like GPT-4 (Generative Pre-trained Transformer 4) work based on a transformer architecture. \n",
    "Here’s a simplified explanation of how they Work:\n",
    "\n",
    "* Learning from Lots of Text: These models start by reading a massive amount of text from the internet. It’s like learning from a giant library of information.\n",
    "\n",
    "* Innovative Architecture: They use a unique structure called a transformer, which helps them understand and remember lots of information.\n",
    "\n",
    "* Breaking Down Words: They look at sentences in smaller parts, like breaking words into pieces. This helps them work with language more efficiently.\n",
    "\n",
    "* Understanding Words in Sentences: Unlike simple programs, these models understand individual words and how words relate to each other in a sentence. They get the whole picture.\n",
    "\n",
    "* Getting Specialized: After the general learning, they can be trained more on specific topics to get good at certain things, like answering questions or writing about particular subjects.\n",
    "\n",
    "* Doing Tasks: When you give them a prompt (a question or instruction), they use what they’ve learned to respond. It’s like having an intelligent assistant that can understand and generate text.\n",
    "\n",
    "General Architecture\n",
    "\n",
    "The architecture of Large Language Model primarily consists of multiple layers of neural networks, like recurrent layers, feedforward layers, embedding layers, and attention layers. \n",
    "These layers work together to process the input text and generate output predictions.\n",
    "\n",
    "* The embedding layer converts each word in the input text into a high-dimensional vector representation. These embeddings capture semantic and syntactic information about the words and help the model to understand the context.\n",
    "\n",
    "* The feedforward layers of Large Language Models have multiple fully connected layers that apply nonlinear transformations to the input embeddings. These layers help the model learn higher-level abstractions from the input text.\n",
    "\n",
    "* The recurrent layers of LLMs are designed to interpret information from the input text in sequence. These layers maintain a hidden state that is updated at each time step, allowing the model to capture the dependencies between words in a sentence.\n",
    "\n",
    "* The attention mechanism is another important part of LLMs, which allows the model to focus selectively on different parts of the input text. This mechanism helps the model attend to the input text’s most relevant parts and generate more accurate predictions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['article']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_template = \"\"\"\n",
    "Article:\n",
    "  \n",
    "{article}\n",
    "\n",
    "----\n",
    "\n",
    "You will generate increasingly concise, entity-dense summaries of the above\n",
    "Article.\n",
    "\n",
    "Repeat the following 2 steps 5 times.\n",
    "\n",
    "- Step 1: Identify 1-3 informative Entities from the Article\n",
    "which are missing from the previously generated summary and are the most\n",
    "relevant.\n",
    "\n",
    "- Step 2: Write a new, denser summary of identical length which covers\n",
    "every entity and detail from the previous summary plus the missing entities\n",
    "\n",
    "A Missing Entity is:\n",
    "\n",
    "- Relevant: to the main story\n",
    "- Specific: descriptive yet concise (5 words or fewer)\n",
    "- Novel: not in the previous summary\n",
    "- Faithful: present in the Article\n",
    "- Anywhere: located anywhere in the Article\n",
    "\n",
    "Guidelines:\n",
    "- The first summary should be long (4-5 sentences, approx. 80 words) yet\n",
    "highly non-specific, containing little information beyond the entities\n",
    "marked as missing.\n",
    "\n",
    "- Use overly verbose language and fillers (e.g. \"this article discusses\") to\n",
    "reach approx. 80 words.\n",
    "\n",
    "- Make every word count: re-write the previous summary to improve flow and\n",
    "make space for additional entities.\n",
    "\n",
    "- Make space with fusion, compression, and removal of uninformative phrases\n",
    "like \"the article discusses\"\n",
    "\n",
    "- The summaries should become highly dense and concise yet self-contained,\n",
    "e.g., easily understood without the Article.\n",
    "\n",
    "- Missing entities can appear anywhere in the new summary.\n",
    "\n",
    "- Never drop entities from the previous summary. If space cannot be made,\n",
    "add fewer new entities.\n",
    "\n",
    "> Remember to use the exact same number of words for each summary.\n",
    "Answer in JSON.\n",
    "\n",
    "> The JSON in `summaries_per_step` should be a list (length 5) of\n",
    "dictionaries whose keys are \"missing_entities\" and \"denser_summary\".\n",
    "\"\"\"\n",
    "\n",
    "article_prompt = PromptTemplate.from_template(article_template)\n",
    "article_prompt.input_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "introduction_template = \"\"\"You are an {person} in writing rich and dense summaries in broad domains.\"\"\"\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)\n",
    "introduction_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompts = [\n",
    "    (\"introductions\", introduction_prompt),\n",
    "    (\"articles\", article_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['articles', 'introductions']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_template = \"\"\"{introductions}\n",
    "\n",
    "{articles}\n",
    "\n",
    "\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "full_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=full_prompt, pipeline_prompts=input_prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['article', 'person']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in writing rich and dense summaries in broad domains.\n",
      "\n",
      "\n",
      "Article:\n",
      "  \n",
      "\n",
      "What is a Large Language Model (LLM)?\n",
      "\n",
      "A large language model is an advanced type of language model that is trained using deep learning techniques on massive amounts of text data. \n",
      "These models are capable of generating human-like text and performing various natural language processing tasks.\n",
      "\n",
      "In contrast, the definition of a language model refers to the concept of assigning probabilities to sequences of words, based on the analysis of text corpora. \n",
      "A language model can be of varying complexity, from simple n-gram models to more sophisticated neural network models. \n",
      "However, the term “large language model” usually refers to models that use deep learning techniques and have a large number of parameters, which can range from millions to billions. \n",
      "These models can capture complex patterns in language and produce text that is often indistinguishable from that written by humans.\n",
      "\n",
      "How a Large Language Model (LLM) Is Built?\n",
      "\n",
      "A large-scale transformer model known as a “large language model” is typically too massive to run on a single computer and is, therefore, provided as a service over an API or web interface. \n",
      "These models are trained on vast amounts of text data from sources such as books, articles, websites, and numerous other forms of written content. \n",
      "By analyzing the statistical relationships between words, phrases, and sentences through this training process, the models can generate coherent and contextually relevant responses to prompts or queries.\n",
      "ChatGPT’s GPT-4, a large language model, was trained on massive amounts of internet text data, allowing it to understand various languages and possess knowledge of diverse topics. \n",
      "As a result, it can produce text in multiple styles. \n",
      "While its capabilities, including translation, text summarization, and question-answering, may seem impressive, they are not surprising, given that these functions operate using special “grammars” that match up with prompts.\n",
      "\n",
      "How do large language models work?\n",
      "\n",
      "Large language models like GPT-4 (Generative Pre-trained Transformer 4) work based on a transformer architecture. \n",
      "Here’s a simplified explanation of how they Work:\n",
      "\n",
      "* Learning from Lots of Text: These models start by reading a massive amount of text from the internet. It’s like learning from a giant library of information.\n",
      "\n",
      "* Innovative Architecture: They use a unique structure called a transformer, which helps them understand and remember lots of information.\n",
      "\n",
      "* Breaking Down Words: They look at sentences in smaller parts, like breaking words into pieces. This helps them work with language more efficiently.\n",
      "\n",
      "* Understanding Words in Sentences: Unlike simple programs, these models understand individual words and how words relate to each other in a sentence. They get the whole picture.\n",
      "\n",
      "* Getting Specialized: After the general learning, they can be trained more on specific topics to get good at certain things, like answering questions or writing about particular subjects.\n",
      "\n",
      "* Doing Tasks: When you give them a prompt (a question or instruction), they use what they’ve learned to respond. It’s like having an intelligent assistant that can understand and generate text.\n",
      "\n",
      "General Architecture\n",
      "\n",
      "The architecture of Large Language Model primarily consists of multiple layers of neural networks, like recurrent layers, feedforward layers, embedding layers, and attention layers. \n",
      "These layers work together to process the input text and generate output predictions.\n",
      "\n",
      "* The embedding layer converts each word in the input text into a high-dimensional vector representation. These embeddings capture semantic and syntactic information about the words and help the model to understand the context.\n",
      "\n",
      "* The feedforward layers of Large Language Models have multiple fully connected layers that apply nonlinear transformations to the input embeddings. These layers help the model learn higher-level abstractions from the input text.\n",
      "\n",
      "* The recurrent layers of LLMs are designed to interpret information from the input text in sequence. These layers maintain a hidden state that is updated at each time step, allowing the model to capture the dependencies between words in a sentence.\n",
      "\n",
      "* The attention mechanism is another important part of LLMs, which allows the model to focus selectively on different parts of the input text. This mechanism helps the model attend to the input text’s most relevant parts and generate more accurate predictions.\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "You will generate increasingly concise, entity-dense summaries of the above\n",
      "Article.\n",
      "\n",
      "Repeat the following 2 steps 5 times.\n",
      "\n",
      "- Step 1: Identify 1-3 informative Entities from the Article\n",
      "which are missing from the previously generated summary and are the most\n",
      "relevant.\n",
      "\n",
      "- Step 2: Write a new, denser summary of identical length which covers\n",
      "every entity and detail from the previous summary plus the missing entities\n",
      "\n",
      "A Missing Entity is:\n",
      "\n",
      "- Relevant: to the main story\n",
      "- Specific: descriptive yet concise (5 words or fewer)\n",
      "- Novel: not in the previous summary\n",
      "- Faithful: present in the Article\n",
      "- Anywhere: located anywhere in the Article\n",
      "\n",
      "Guidelines:\n",
      "- The first summary should be long (4-5 sentences, approx. 80 words) yet\n",
      "highly non-specific, containing little information beyond the entities\n",
      "marked as missing.\n",
      "\n",
      "- Use overly verbose language and fillers (e.g. \"this article discusses\") to\n",
      "reach approx. 80 words.\n",
      "\n",
      "- Make every word count: re-write the previous summary to improve flow and\n",
      "make space for additional entities.\n",
      "\n",
      "- Make space with fusion, compression, and removal of uninformative phrases\n",
      "like \"the article discusses\"\n",
      "\n",
      "- The summaries should become highly dense and concise yet self-contained,\n",
      "e.g., easily understood without the Article.\n",
      "\n",
      "- Missing entities can appear anywhere in the new summary.\n",
      "\n",
      "- Never drop entities from the previous summary. If space cannot be made,\n",
      "add fewer new entities.\n",
      "\n",
      "> Remember to use the exact same number of words for each summary.\n",
      "Answer in JSON.\n",
      "\n",
      "> The JSON in `summaries_per_step` should be a list (length 5) of\n",
      "dictionaries whose keys are \"missing_entities\" and \"denser_summary\".\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    pipeline_prompt.format(\n",
    "        person=\"expert\",\n",
    "        article=article,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "chain = pipeline_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"person\": \"expert\", \"article\": article})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "summaries_per_step": [
        {
         "denser_summary": "This article discusses the concept of a Large Language Model (LLM), a form of language model. Utilizing deep learning techniques, these models are trained on vast text data, capable of generating human-like text and performing various natural language processing tasks. The unprecedented complexity of LLMs allows them to capture complex patterns in language.",
         "missing_entities": [
          "Large Language Model (LLM)",
          "Language model",
          "Deep learning techniques"
         ]
        },
        {
         "denser_summary": "A Large Language Model (LLM) like GPT-4, influenced by transformer architecture, uses deep learning techniques for training on extensive text data. These models generate human-like text and handle various natural language processing tasks, capturing intricate language patterns. The sheer complexity of LLMs enables them to interpret and generate text indistinguishable from human-produced content.",
         "missing_entities": [
          "Transformer architecture",
          "GPT-4"
         ]
        },
        {
         "denser_summary": "GPT-4, an example of a Large Language Model (LLM) influenced by transformer architecture, uses deep learning techniques and neural network layers for training on extensive text data. The embedding layer transforms input text into high-dimensional vectors. These models generate human-like text, handle various language processing tasks, and capture intricate language patterns, producing text indistinguishable from human-written content.",
         "missing_entities": [
          "Neural network layers",
          "Embedding layer"
         ]
        },
        {
         "denser_summary": "GPT-4, a Large Language Model (LLM) with transformer architecture, employs deep learning techniques, neural network layers, including feedforward and recurrent layers, and an embedding layer for training on substantial text data. The embedding layer converts input into high-dimensional vectors, making the model capable of generating human-like text, handling various language processing tasks, and capturing intricate language patterns.",
         "missing_entities": [
          "Feedforward layers",
          "Recurrent layers"
         ]
        },
        {
         "denser_summary": "GPT-4, a Large Language Model (LLM) leveraging transformer architecture, uses deep learning techniques, neural network layers (including feedforward and recurrent layers), an embedding layer, and an attention mechanism for training on substantial text data and generating output predictions. The model translates input into high-dimensional vectors, enabling the generation of human-like text, handling of various language processing tasks, and capturing of intricate language patterns.",
         "missing_entities": [
          "Attention mechanism",
          "Output predictions"
         ]
        }
       ]
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": true,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(JSON(output, expanded=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"summaries_per_step\": [\n",
      "    {\n",
      "      \"missing_entities\": [\n",
      "        \"Large Language Model (LLM)\",\n",
      "        \"Language model\",\n",
      "        \"Deep learning techniques\"\n",
      "      ],\n",
      "      \"denser_summary\": \"This article discusses the concept of a Large Language Model (LLM), a form of language model. Utilizing deep learning techniques, these models are trained on vast text data, capable of generating human-like text and performing various natural language processing tasks. The unprecedented complexity of LLMs allows them to capture complex patterns in language.\"\n",
      "    },\n",
      "    {\n",
      "      \"missing_entities\": [\n",
      "        \"Transformer architecture\",\n",
      "        \"GPT-4\"\n",
      "      ],\n",
      "      \"denser_summary\": \"A Large Language Model (LLM) like GPT-4, influenced by transformer architecture, uses deep learning techniques for training on extensive text data. These models generate human-like text and handle various natural language processing tasks, capturing intricate language patterns. The sheer complexity of LLMs enables them to interpret and generate text indistinguishable from human-produced content.\"\n",
      "    },\n",
      "    {\n",
      "      \"missing_entities\": [\n",
      "        \"Neural network layers\",\n",
      "        \"Embedding layer\"\n",
      "      ],\n",
      "      \"denser_summary\": \"GPT-4, an example of a Large Language Model (LLM) influenced by transformer architecture, uses deep learning techniques and neural network layers for training on extensive text data. The embedding layer transforms input text into high-dimensional vectors. These models generate human-like text, handle various language processing tasks, and capture intricate language patterns, producing text indistinguishable from human-written content.\"\n",
      "    },\n",
      "    {\n",
      "      \"missing_entities\": [\n",
      "        \"Feedforward layers\",\n",
      "        \"Recurrent layers\"\n",
      "      ],\n",
      "      \"denser_summary\": \"GPT-4, a Large Language Model (LLM) with transformer architecture, employs deep learning techniques, neural network layers, including feedforward and recurrent layers, and an embedding layer for training on substantial text data. The embedding layer converts input into high-dimensional vectors, making the model capable of generating human-like text, handling various language processing tasks, and capturing intricate language patterns.\"\n",
      "    },\n",
      "    {\n",
      "      \"missing_entities\": [\n",
      "        \"Attention mechanism\",\n",
      "        \"Output predictions\"\n",
      "      ],\n",
      "      \"denser_summary\": \"GPT-4, a Large Language Model (LLM) leveraging transformer architecture, uses deep learning techniques, neural network layers (including feedforward and recurrent layers), an embedding layer, and an attention mechanism for training on substantial text data and generating output predictions. The model translates input into high-dimensional vectors, enabling the generation of human-like text, handling of various language processing tasks, and capturing of intricate language patterns.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(output[\"summaries_per_step\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This article discusses the concept of a Large Language Model (LLM), a form of language model. Utilizing deep learning techniques, these models are trained on vast text data, capable of generating human-like text and performing various natural language processing tasks. The unprecedented complexity of LLMs allows them to capture complex patterns in language."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(output[\"summaries_per_step\"][0][\"denser_summary\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Large Language Model (LLM)', 'Language model', 'Deep learning techniques']\n"
     ]
    }
   ],
   "source": [
    "print(output[\"summaries_per_step\"][0][\"missing_entities\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-verification (CoV)\n",
    "\n",
    "In the recently released paper [Chain-of-Verification Reduces Hallucination in Large Language Models](https://arxiv.org/abs/2309.11495), the authors show how Chain-of-Verification (CoVe) can reduce hallucination through a 4-steps process:\n",
    "\n",
    "1. Generate baseline response (query LLM)\n",
    "1. Plan verifications (given query and baseline response, generate a list of questions that helps verifying any mistakes in the original response)\n",
    "1. Execute verifications (answer each verification question, check against original response for inconsistency / mistakes)\n",
    "1. General final verified response (generate a revised response incorporating the results from the verification step if there are any inconsistencies)\n",
    "\n",
    "More details here https://medium.com/@james.li/a-langchain-implementation-of-chain-of-verification-cove-to-reduce-hallucination-0a8fa2929b2a and https://mister-seo.com/chatgpt-faktencheck-chain-of-verification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which German chancellors were born in Berlin?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate baseline respose chain\n",
    "\n",
    "input_variables = [\"query\"]\n",
    "base_response_output_key = \"base_response\"\n",
    "base_response_template = \"\"\"Question: {query} Answer:\"\"\"\n",
    "base_repsonse_prompt_template = PromptTemplate(\n",
    "    input_variables=input_variables, template=base_response_template\n",
    ")\n",
    "\n",
    "base_response_chain = LLMChain(\n",
    "    llm=llm, prompt=base_repsonse_prompt_template, output_key=base_response_output_key\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Plan verification chain\n",
    "\n",
    "plan_verifications_template = \"\"\"\n",
    "Given the below Question and answer, generate a series of verification questions that test the factual claims in the original baseline response.\n",
    "For example if part of a longform model response contains the statement \n",
    "“Federal Chancellors Willy Brandt and Angela Merkel were born in Berlin. Brandt was born in 1913, Merkel in 1954”, \n",
    "then one possible verification question to check those dates could be “Were Chancellor Willy Brandt and Angela Merkel born in Berlin?”\n",
    "\n",
    "Question: {query}\n",
    "Answer: {base_response}\n",
    "\n",
    "<fact in passage>, <verification question, generated by combining the query and the fact>\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class PlanVerificationsOutput(BaseModel):\n",
    "    query: str = Field(description=\"The user's query\")\n",
    "    base_response: str = Field(description=\"The response to the user's query\")\n",
    "    facts_and_verification_questions: dict[str, str] = Field(\n",
    "        description=\"Facts (as the dictionary keys) extracted from the response and verification questions related to the query (as the dictionary values)\"\n",
    "    )\n",
    "\n",
    "\n",
    "plan_verifications_output_parser = PydanticOutputParser(\n",
    "    pydantic_object=PlanVerificationsOutput\n",
    ")\n",
    "\n",
    "plan_verifications_prompt_template = PromptTemplate(\n",
    "    input_variables=input_variables + [base_response_output_key],\n",
    "    template=plan_verifications_template,\n",
    "    partial_variables={\n",
    "        \"format_instructions\": plan_verifications_output_parser.get_format_instructions()\n",
    "    },\n",
    ")\n",
    "plan_verifications_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=plan_verifications_prompt_template,\n",
    "    output_key=\"output\",\n",
    "    output_parser=plan_verifications_output_parser,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramon.wartala/Documents/source/conversational-apps-with-langchain/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 3. Baseline + Plan verification sequential chain\n",
    "\n",
    "answer_and_plan_verification = SequentialChain(\n",
    "    chains=[base_response_chain, plan_verifications_chain],\n",
    "    input_variables=[\"query\"],\n",
    "    output_variables=[\"output\"],\n",
    "    verbose=True)\n",
    "\n",
    "\n",
    "intermediate_result = answer_and_plan_verification.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Willy Brandt and Angela Merkel.'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_result.base_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Willy Brandt was born in Berlin': 'Was Willy Brandt born in Berlin?',\n",
       " 'Angela Merkel was born in Berlin': 'Was Angela Merkel born in Berlin?'}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_result.facts_and_verification_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "claimed_facts = list(intermediate_result.facts_and_verification_questions.keys())\n",
    "verification_questions = list(\n",
    "    intermediate_result.facts_and_verification_questions.values()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_results_str = \"\"\n",
    "verify_input_variables = [\"question\"]\n",
    "verify_output_key = \"answer\"\n",
    "verify_template = \"\"\"{question}\"\"\"\n",
    "\n",
    "verify_prompt_template = PromptTemplate(\n",
    "    input_variables=verify_input_variables, template=verify_template\n",
    ")\n",
    "\n",
    "verify_chain = LLMChain(\n",
    "    llm=llm, prompt=verify_prompt_template, output_key=verify_output_key\n",
    ")\n",
    "for i in range(len(verification_questions)):\n",
    "    claimed_fact = claimed_facts[i]\n",
    "    question = verification_questions[i]\n",
    "    answer = verify_chain.run(question)\n",
    "    answer = answer.lstrip(\"\\n\")\n",
    "    verify_results_str += f\"Question: {question}\\nAnswer: {answer}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Was Willy Brandt born in Berlin?\n",
      "Answer: No, Willy Brandt was born in Lübeck, Germany.\n",
      "\n",
      "Question: Was Angela Merkel born in Berlin?\n",
      "Answer: No, Angela Merkel was born in Hamburg, West Germany.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(verify_results_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-of_Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
