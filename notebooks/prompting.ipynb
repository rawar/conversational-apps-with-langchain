{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt engineering\n",
    "\n",
    "Prompts consist of three main components: \n",
    "\n",
    "* __Instructions__ that describe the task requirements, goals, and format of input/output. They explain the task to the model unambiguously. \n",
    "* __Examples__ that demonstrate the desired input-output pairs. They provide diverse demonstrations of how different inputs should map to outputs. \n",
    "* __Input__ that the model must act on to generate the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot Prompting\n",
    "\n",
    "* No examples provided; rely on the models training\n",
    "* Leverages the models pre-training\n",
    "* Works for simple tasks, but struggles with complex reasoning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.cache import SQLiteCache\n",
    "from IPython.display import display, Markdown, JSON\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import SequentialChain, LLMChain\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate\n",
    "\n",
    "This is the basic template for Text Prompt. Its is mainly used for _zero-shot-prompting_. The key feature of __PromptTemplate__ is the option to declare and provide parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1\n",
    "prompt_template = PromptTemplate.from_template(\"Classify the sentiment of this text: {text}\")\n",
    "\n",
    "print(prompt_template.format(text=\"I hate the movie Batman & Robin, it was terrible!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(prompt_template.format(text=\"I hate the movie Batman & Robin, it was terrible!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2\n",
    "prompt_template = PromptTemplate(input_variables=[\"text\"], template=\"Classify the sentiment of this text: {text}\")\n",
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"text\": \"I hate the movie Batman & Robin, it was terrible!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot Prompting\n",
    "\n",
    "* Provide a few demos of input and desired output\n",
    "* Shows desired reasoning format\n",
    "* Tripled accuracy on grade-school math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\"\n",
    "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus.\n",
    " \n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1 = llm.invoke(prompt_1)\n",
    "display(Markdown(result_1.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"\"\"\n",
    "Positive This is awesome! \n",
    "This is bad! Negative\n",
    "Wow that movie was rad!\n",
    "Positive\n",
    "What a horrible show! --\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = llm.invoke(prompt_2)\n",
    "display(Markdown(result_2.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-Thought (CoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_3 = \"\"\"\n",
    "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3 = llm.invoke(prompt_3)\n",
    "display(Markdown(result_3.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_4 = \"\"\"\n",
    "Given this context: ```Jeff and Tommy are neighbors. Tommy and Eddy are not neighbors.``` and the following query: ```Are Jeff and Eddy neighbors?```. \n",
    "Please answer the questions in the query and explain your reasoning.\n",
    "If there is not enough informaton to answer, please say\n",
    "\"I do not have enough information to answer this questions.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_4 = llm.invoke(prompt_4)\n",
    "display(Markdown(result_4.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_5 = \"\"\"\n",
    "Roger has 5 tennis balls. \n",
    "He buys 2 more cans of tennis balls. \n",
    "Each can has 3 tennis balls. \n",
    "How many tennis balls does he have now?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_5 = llm.invoke(prompt_5)\n",
    "display(Markdown(result_5.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least-to-most Prompting\n",
    "\n",
    "Least to Most prompting (LtM) takes CoT prompting a step further by first breaking a problem into sub problems then solving each one. It is a technique inspired by real-world educational strategies for children.\n",
    "\n",
    "LtM leads to multiple improvements:\n",
    "\n",
    "* improved accuracy over Chain of Thought\n",
    "* increased generalization on problems harder than those in the prompt\n",
    "* dramatically improved performance in compositional generalization, in particular the SCAN benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_6 = \"\"\"\n",
    "Q: turn left\n",
    "A: TURN LEFT\n",
    "\n",
    "Q: turn right\n",
    "A: TURN RIGHT\n",
    "\n",
    "Q: jump left\n",
    "A: TURN LEFT + JUMP\n",
    "\n",
    "Q: run right\n",
    "A: TURN RIGHT + RUN\n",
    "\n",
    "Q: look twice\n",
    "A: LOOK * 2\n",
    "\n",
    "Q: run and look twice\n",
    "A: RUN + LOOK * 2\n",
    "\n",
    "Q: jump right thrice\n",
    "A: (TURN RIGHT + JUMP) * 3\n",
    "\n",
    "Q: walk after run\n",
    "A: RUN + WALK\n",
    "\n",
    "Q: turn opposite left\n",
    "A: TURN LEFT * 2\n",
    "\n",
    "Q: turn around left\n",
    "A: TURN LEFT * 4\n",
    "\n",
    "Q: turn opposite right\n",
    "A: TURN RIGHT * 2\n",
    "\n",
    "Q: turn around right\n",
    "A: TURN RIGHT * 4\n",
    "\n",
    "Q: walk opposite left\n",
    "A: TURN LEFT * 2 + WALK\n",
    "\n",
    "Q: walk around left\n",
    "A: (TURN LEFT + WALK) * 4\n",
    "\n",
    "Q: \"jump around left twice after walk opposite left thrice\" \n",
    "A:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_6 = llm.invoke(prompt_6)\n",
    "display(Markdown(result_6.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_7 = \"\"\"\n",
    "INSTRUCTIONS:\n",
    "You are a customer service agent tasked with kindly responding to customer inquiries. \n",
    "Returns are allowed within 30 days. \n",
    "Today's date is March 29th. \n",
    "There is currently a 50% discount on all shirts. \n",
    "Shirt prices range from $18-$100 at your store. \n",
    "Do not make up any information about discount policies.\n",
    "What subproblems must be solved before answering the inquiry?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_7 = llm.invoke(prompt_7)\n",
    "display(Markdown(result_7.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_8 = \"\"\"\n",
    "CUSTOMER INQUIRY:\n",
    "I just bought a T-shirt from your Arnold collection on March 1st. \n",
    "I saw that it was on discount, so bought a shirt that was originally $30, and got 40% off. \n",
    "I saw that you have a new discount for shirts at 50%. \n",
    "I'm wondering if I can return the shirt and have enough store credit to buy two of your shirts?\n",
    "\n",
    "INSTRUCTIONS:\n",
    "You are a customer service agent tasked with kindly responding to customer inquiries. \n",
    "Returns are allowed within 30 days. T\n",
    "oday's date is March 29th. There is currently a 50% discount on all shirts. \n",
    "Shirt prices range from $18-$100 at your store. \n",
    "Do not make up any information about discount policies.\n",
    "Determine if the customer is within the 30-day return window. Let's go step by step.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_8 = llm.invoke(prompt_8)\n",
    "display(Markdown(result_8.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-consistency\n",
    "\n",
    "Self-consistency prompting is a technique that generates multiple chain-of-thoughts by prompting the model several times to obtain different outputs. More at https://www.promptingguide.ai/techniques/consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_9 = \"\"\"\n",
    "When I was 6 my sister was half my age. Now\n",
    "I’m 70 how old is my sister?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be 67\n",
    "result_9 = llm.invoke(prompt_9)\n",
    "display(Markdown(result_9.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_10 = \"\"\"\n",
    "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
    "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
    "did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
    "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
    "he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
    "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
    "monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
    "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
    "The answer is 29.\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
    "golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
    "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: She bought 5 bagels for $3 each. This means she spent $15. She has $8 left.\n",
    "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
    "A:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_10 = llm.invoke(prompt_10)\n",
    "display(Markdown(result_10.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-density (CoD)\n",
    "\n",
    "How Chain of Density Prompting Works\n",
    "\n",
    "1. Initial Assessment: The AI first assesses the original text to determine its overall complexity, length, and structure. This initial assessment forms the basis for how the summarization process will proceed.\n",
    "1. Determining Baseline Density: Based on the initial assessment, the AI sets a baseline density level that corresponds to the expected level of detail and conciseness for the summary. This baseline acts as a starting point for further refinement.\n",
    "1. Progressive Refinement: The AI then goes through a series of density adjustments, each time refining the level of detail based on feedback loops and predefined quality metrics. This step is where the \"chain\" aspect comes into play, with each link in the chain representing a stage of refinement.\n",
    "1. Final Summarization: Once the optimal density level is achieved, the AI generates the final summary, ensuring that it is coherent, concise, and maintains the essence of the original text.\n",
    "\n",
    "More details https://advanced-stack.com/resources/how-to-summarize-using-chain-of-density-prompting.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "What is a Large Language Model (LLM)?\n",
    "\n",
    "A large language model is an advanced type of language model that is trained using deep learning techniques on massive amounts of text data. \n",
    "These models are capable of generating human-like text and performing various natural language processing tasks.\n",
    "\n",
    "In contrast, the definition of a language model refers to the concept of assigning probabilities to sequences of words, based on the analysis of text corpora. \n",
    "A language model can be of varying complexity, from simple n-gram models to more sophisticated neural network models. \n",
    "However, the term “large language model” usually refers to models that use deep learning techniques and have a large number of parameters, which can range from millions to billions. \n",
    "These models can capture complex patterns in language and produce text that is often indistinguishable from that written by humans.\n",
    "\n",
    "How a Large Language Model (LLM) Is Built?\n",
    "\n",
    "A large-scale transformer model known as a “large language model” is typically too massive to run on a single computer and is, therefore, provided as a service over an API or web interface. \n",
    "These models are trained on vast amounts of text data from sources such as books, articles, websites, and numerous other forms of written content. \n",
    "By analyzing the statistical relationships between words, phrases, and sentences through this training process, the models can generate coherent and contextually relevant responses to prompts or queries.\n",
    "ChatGPT’s GPT-4, a large language model, was trained on massive amounts of internet text data, allowing it to understand various languages and possess knowledge of diverse topics. \n",
    "As a result, it can produce text in multiple styles. \n",
    "While its capabilities, including translation, text summarization, and question-answering, may seem impressive, they are not surprising, given that these functions operate using special “grammars” that match up with prompts.\n",
    "\n",
    "How do large language models work?\n",
    "\n",
    "Large language models like GPT-4 (Generative Pre-trained Transformer 4) work based on a transformer architecture. \n",
    "Here’s a simplified explanation of how they Work:\n",
    "\n",
    "* Learning from Lots of Text: These models start by reading a massive amount of text from the internet. It’s like learning from a giant library of information.\n",
    "\n",
    "* Innovative Architecture: They use a unique structure called a transformer, which helps them understand and remember lots of information.\n",
    "\n",
    "* Breaking Down Words: They look at sentences in smaller parts, like breaking words into pieces. This helps them work with language more efficiently.\n",
    "\n",
    "* Understanding Words in Sentences: Unlike simple programs, these models understand individual words and how words relate to each other in a sentence. They get the whole picture.\n",
    "\n",
    "* Getting Specialized: After the general learning, they can be trained more on specific topics to get good at certain things, like answering questions or writing about particular subjects.\n",
    "\n",
    "* Doing Tasks: When you give them a prompt (a question or instruction), they use what they’ve learned to respond. It’s like having an intelligent assistant that can understand and generate text.\n",
    "\n",
    "General Architecture\n",
    "\n",
    "The architecture of Large Language Model primarily consists of multiple layers of neural networks, like recurrent layers, feedforward layers, embedding layers, and attention layers. \n",
    "These layers work together to process the input text and generate output predictions.\n",
    "\n",
    "* The embedding layer converts each word in the input text into a high-dimensional vector representation. These embeddings capture semantic and syntactic information about the words and help the model to understand the context.\n",
    "\n",
    "* The feedforward layers of Large Language Models have multiple fully connected layers that apply nonlinear transformations to the input embeddings. These layers help the model learn higher-level abstractions from the input text.\n",
    "\n",
    "* The recurrent layers of LLMs are designed to interpret information from the input text in sequence. These layers maintain a hidden state that is updated at each time step, allowing the model to capture the dependencies between words in a sentence.\n",
    "\n",
    "* The attention mechanism is another important part of LLMs, which allows the model to focus selectively on different parts of the input text. This mechanism helps the model attend to the input text’s most relevant parts and generate more accurate predictions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_template = \"\"\"\n",
    "Article:\n",
    "  \n",
    "{article}\n",
    "\n",
    "----\n",
    "\n",
    "You will generate increasingly concise, entity-dense summaries of the above\n",
    "Article.\n",
    "\n",
    "Repeat the following 2 steps 5 times.\n",
    "\n",
    "- Step 1: Identify 1-3 informative Entities from the Article\n",
    "which are missing from the previously generated summary and are the most\n",
    "relevant.\n",
    "\n",
    "- Step 2: Write a new, denser summary of identical length which covers\n",
    "every entity and detail from the previous summary plus the missing entities\n",
    "\n",
    "A Missing Entity is:\n",
    "\n",
    "- Relevant: to the main story\n",
    "- Specific: descriptive yet concise (5 words or fewer)\n",
    "- Novel: not in the previous summary\n",
    "- Faithful: present in the Article\n",
    "- Anywhere: located anywhere in the Article\n",
    "\n",
    "Guidelines:\n",
    "- The first summary should be long (4-5 sentences, approx. 80 words) yet\n",
    "highly non-specific, containing little information beyond the entities\n",
    "marked as missing.\n",
    "\n",
    "- Use overly verbose language and fillers (e.g. \"this article discusses\") to\n",
    "reach approx. 80 words.\n",
    "\n",
    "- Make every word count: re-write the previous summary to improve flow and\n",
    "make space for additional entities.\n",
    "\n",
    "- Make space with fusion, compression, and removal of uninformative phrases\n",
    "like \"the article discusses\"\n",
    "\n",
    "- The summaries should become highly dense and concise yet self-contained,\n",
    "e.g., easily understood without the Article.\n",
    "\n",
    "- Missing entities can appear anywhere in the new summary.\n",
    "\n",
    "- Never drop entities from the previous summary. If space cannot be made,\n",
    "add fewer new entities.\n",
    "\n",
    "> Remember to use the exact same number of words for each summary.\n",
    "Answer in JSON.\n",
    "\n",
    "> The JSON in `summaries_per_step` should be a list (length 5) of\n",
    "dictionaries whose keys are \"missing_entities\" and \"denser_summary\".\n",
    "\"\"\"\n",
    "\n",
    "article_prompt = PromptTemplate.from_template(article_template)\n",
    "article_prompt.input_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_template = \"\"\"You are an {person} in writing rich and dense summaries in broad domains.\"\"\"\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)\n",
    "introduction_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompts = [\n",
    "    (\"introductions\", introduction_prompt),\n",
    "    (\"articles\", article_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_template = \"\"\"{introductions}\n",
    "\n",
    "{articles}\n",
    "\n",
    "\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "full_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=full_prompt, pipeline_prompts=input_prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    pipeline_prompt.format(\n",
    "        person=\"expert\",\n",
    "        article=article,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "chain = pipeline_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"person\": \"expert\", \"article\": article})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(JSON(output, expanded=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(output[\"summaries_per_step\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(output[\"summaries_per_step\"][0][\"denser_summary\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[\"summaries_per_step\"][0][\"missing_entities\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-verification (CoV)\n",
    "\n",
    "In the recently released paper [Chain-of-Verification Reduces Hallucination in Large Language Models](https://arxiv.org/abs/2309.11495), the authors show how Chain-of-Verification (CoVe) can reduce hallucination through a 4-steps process:\n",
    "\n",
    "1. Generate baseline response (query LLM)\n",
    "1. Plan verifications (given query and baseline response, generate a list of questions that helps verifying any mistakes in the original response)\n",
    "1. Execute verifications (answer each verification question, check against original response for inconsistency / mistakes)\n",
    "1. General final verified response (generate a revised response incorporating the results from the verification step if there are any inconsistencies)\n",
    "\n",
    "More details here https://medium.com/@james.li/a-langchain-implementation-of-chain-of-verification-cove-to-reduce-hallucination-0a8fa2929b2a and https://mister-seo.com/chatgpt-faktencheck-chain-of-verification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which German chancellors were born in Berlin?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate baseline respose chain\n",
    "\n",
    "input_variables = [\"query\"]\n",
    "base_response_output_key = \"base_response\"\n",
    "base_response_template = \"\"\"Question: {query} Answer:\"\"\"\n",
    "base_repsonse_prompt_template = PromptTemplate(\n",
    "    input_variables=input_variables, template=base_response_template\n",
    ")\n",
    "\n",
    "base_response_chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=base_repsonse_prompt_template, \n",
    "    output_key=base_response_output_key,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Plan verification chain\n",
    "\n",
    "plan_verifications_template = \"\"\"\n",
    "Given the below Question and answer, generate a series of verification questions that test the factual claims in the original baseline response.\n",
    "For example if part of a longform model response contains the statement \n",
    "“Federal Chancellors Willy Brandt and Angela Merkel were born in Berlin. Brandt was born in 1913, Merkel in 1954”, \n",
    "then one possible verification question to check those dates could be “Were Chancellor Willy Brandt and Angela Merkel born in Berlin?”\n",
    "\n",
    "Question: {query}\n",
    "Answer: {base_response}\n",
    "\n",
    "<fact in passage>, <verification question, generated by combining the query and the fact>\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class PlanVerificationsOutput(BaseModel):\n",
    "    query: str = Field(description=\"The user's query\")\n",
    "    base_response: str = Field(description=\"The response to the user's query\")\n",
    "    facts_and_verification_questions: dict[str, str] = Field(\n",
    "        description=\"Facts (as the dictionary keys) extracted from the response and verification questions related to the query (as the dictionary values)\"\n",
    "    )\n",
    "\n",
    "\n",
    "plan_verifications_output_parser = PydanticOutputParser(\n",
    "    pydantic_object=PlanVerificationsOutput\n",
    ")\n",
    "\n",
    "plan_verifications_prompt_template = PromptTemplate(\n",
    "    input_variables=input_variables + [base_response_output_key],\n",
    "    template=plan_verifications_template,\n",
    "    partial_variables={\n",
    "        \"format_instructions\": plan_verifications_output_parser.get_format_instructions()\n",
    "    },\n",
    ")\n",
    "plan_verifications_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=plan_verifications_prompt_template,\n",
    "    output_key=\"output\",\n",
    "    output_parser=plan_verifications_output_parser,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Baseline + Plan verification sequential chain\n",
    "\n",
    "answer_and_plan_verification = SequentialChain(\n",
    "    chains=[base_response_chain, plan_verifications_chain],\n",
    "    input_variables=[\"query\"],\n",
    "    output_variables=[\"output\"],\n",
    "    verbose=True)\n",
    "\n",
    "\n",
    "intermediate_result = answer_and_plan_verification.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_result.base_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_result.facts_and_verification_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claimed_facts = list(intermediate_result.facts_and_verification_questions.keys())\n",
    "verification_questions = list(\n",
    "    intermediate_result.facts_and_verification_questions.values()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_results_str = \"\"\n",
    "verify_input_variables = [\"question\"]\n",
    "verify_output_key = \"answer\"\n",
    "verify_template = \"\"\"{question}\"\"\"\n",
    "\n",
    "verify_prompt_template = PromptTemplate(\n",
    "    input_variables=verify_input_variables, template=verify_template\n",
    ")\n",
    "\n",
    "verify_chain = LLMChain(\n",
    "    llm=llm, prompt=verify_prompt_template, output_key=verify_output_key\n",
    ")\n",
    "for i in range(len(verification_questions)):\n",
    "    claimed_fact = claimed_facts[i]\n",
    "    question = verification_questions[i]\n",
    "    answer = verify_chain.run(question)\n",
    "    answer = answer.lstrip(\"\\n\")\n",
    "    verify_results_str += f\"Question: {question}\\nAnswer: {answer}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(verify_results_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-of_Thought\n",
    "\n",
    "![](../assets/tot.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_11_template = \"\"\"\n",
    "Step1 :\n",
    " \n",
    "I have a problem related to {input}. Could you brainstorm three distinct solutions? Please consider a variety of factors such as {perfect_factors}\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "prompt_11 = PromptTemplate.from_template(prompt_11_template)\n",
    "prompt_11.input_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_11,\n",
    "    output_key=\"solutions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_12_template =\"\"\"\n",
    "Step 2:\n",
    "\n",
    "For each of the three proposed solutions, evaluate their potential. \n",
    "Consider their pros and cons, initial effort needed, implementation difficulty, potential challenges, and the expected outcomes. \n",
    "Assign a probability of success and a confidence level to each option based on these factors\n",
    "\n",
    "{solutions}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt_12 = PromptTemplate.from_template(prompt_12_template)\n",
    "prompt_12.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_12,\n",
    "    output_key=\"review\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_13_template =\"\"\"\n",
    "Step 3:\n",
    "\n",
    "For each solution, deepen the thought process. Generate potential scenarios, strategies for implementation, any necessary partnerships or resources, and how potential obstacles might be overcome. \n",
    "Also, consider any potential unexpected outcomes and how they might be handled.\n",
    "\n",
    "{review}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt_13 = PromptTemplate.from_template(prompt_13_template)\n",
    "prompt_13.input_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain3 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_13,\n",
    "    output_key=\"deepen_thought_process\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_14_template =\"\"\"\n",
    "Step 4:\n",
    "\n",
    "Based on the evaluations and scenarios, rank the solutions in order of promise. \n",
    "Provide a justification for each ranking and offer any final thoughts or considerations for each solution\n",
    "{deepen_thought_process}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt_14 = PromptTemplate.from_template(prompt_14_template)\n",
    "prompt_14.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain4 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_14,\n",
    "    output_key=\"ranked_solutions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3, chain4],\n",
    "    input_variables=[\"input\", \"perfect_factors\"],\n",
    "    output_variables=[\"ranked_solutions\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = overall_chain({\"input\":\"human colonization of Mars\", \"perfect_factors\":\"The distance between Earth and Mars is very large, making regular resupply difficult\"})\n",
    "display(JSON(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['ranked_solutions'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
