{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatMemory\n",
    "\n",
    "A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:\n",
    "\n",
    "Simply stuffing previous messages into a chat model prompt.\n",
    "The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n",
    "More complex modifications like synthesizing summaries for long running conversations.\n",
    "\n",
    "https://python.langchain.com/docs/modules/memory/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(base_url=\"http://localhost:11434\", model=\"gemma:7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that \"chat_history\" is present in the prompt template\n",
    "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "New human question: {question}\n",
    "Response:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that we need to align the `memory_key`\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Response:**\n",
      "\n",
      "\"Hello, Ramon. It's a pleasure to hear from you again. I'm glad you decided to chat with me today. What would you like to talk about?\"\n"
     ]
    }
   ],
   "source": [
    "# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
    "response1 = conversation.invoke({\"question\": \"hi, my name is Ramon\"})\n",
    "print(response1['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Response:**\n",
      "\n",
      "\"Your name is Ramon, as you have previously told me. I have not forgotten that.\"\n"
     ]
    }
   ],
   "source": [
    "response2 = conversation.invoke({\"question\": \"What is my name?\"})\n",
    "print(response2['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## New Human Question and Response:\n",
      "\n",
      "**Human:** I like to program Python very much, what is your favorite program language?\n",
      "\n",
      "**AI:** \"I don't have the capacity to program, therefore I don't have a favorite programming language. I am a language model designed to provide you with information and help you with your queries.\"\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* The previous conversation established the human's name as Ramon and the AI's ability to recall that information.\n",
      "* The new question is about the AI's favorite programming language.\n",
      "* The AI's response clarifies its limitations and explains that it doesn't have the ability to program.\n",
      "\n",
      "**Overall, the response is:**\n",
      "\n",
      "* **Appropriate:** It answers the question asked by the human.\n",
      "* **Clear and concise:** It is direct and to the point, avoiding unnecessary details.\n",
      "* **Self-aware:** It acknowledges its limitations and doesn't make false claims.\n"
     ]
    }
   ],
   "source": [
    "response3 = conversation.invoke({\"question\": \"I like to program Python very much, what is your favorite program language?\"})\n",
    "print(response3['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Response:\n",
      "\n",
      "\"I do remember your name, Ramon, and I also remember you mentioning your favorite programming language, Python, in a previous conversation. I have not forgotten that information. Would you like me to tell you more about it?\"\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* This response acknowledges the previous conversation and recalls the human's name and favorite programming language.\n",
      "* It offers to continue the conversation on the topic of the human's favorite programming language.\n",
      "* It is friendly and engaging, inviting the human to engage further.\n",
      "\n",
      "**Overall, this response is:**\n",
      "\n",
      "* **Appropriate:** It answers the question asked by the human.\n",
      "* **Clear and concise:** It is direct and to the point, avoiding unnecessary details.\n",
      "* **Friendly and engaging:** It is friendly and inviting, creating a positive interaction.\n"
     ]
    }
   ],
   "source": [
    "response4 = conversation.invoke({\"question\": \"Do you remember my name and my favorite programming language?\"})\n",
    "print(response4['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
