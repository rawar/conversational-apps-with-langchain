{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Models (LLMs)\n",
    "\n",
    "Large Language Models (LLMs) are a core component of LangChain. LangChain does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs. To be specific, this interface is one that takes as input a string and returns a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "print(prompt.format(product=\"podcast player\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "          model_name=\"text-davinci-003\",    # default model\n",
    "          temperature=0.9                   #temperature dictates how whacky the output should be\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure OpenAI\n",
    "\n",
    "Azure OpenAI Service provides REST API access to OpenAI’s powerful language models including the GPT-4, GPT-3.5-Turbo, and Embeddings model series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = input(\"Enter your Astra DB API Endpoint: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = HumanMessage(\n",
    "    content=\"Translate this sentence from English to French. I love programming.\"\n",
    ")\n",
    "llm([message])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud Vetex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface\n",
    "\n",
    "To instantiate a Hugging Face LLM there are three options available\n",
    "\n",
    "* HuggingFaceTextGenInference\n",
    "* HuggingFaceEndpoint\n",
    "* HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.llms import HuggingFaceTextGenInference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "\n",
    "Ollama allows you to run open-source large language models, such as Google's Gemma, LlaMA 2 or Mistral locally. Ollama supports importing [models](https://ollama.com/library) in GGUF format (GPT-Generated Unified Format). Ollama bundles model weights, configuration, and data into a single package, defined by a Modelfile. It optimizes setup and configuration details, including CPU and GPU usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemma7B\n",
    "\n",
    "Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. It’s inspired by Gemini models at Google.\n",
    "Gemma is available in both 2b and 7b parameter sizes. The models undergo training on a diverse dataset of web documents to expose them to a wide range of linguistic styles, topics, and vocabularies. This includes code to learn syntax and patterns of programming languages, as well as mathematical text to grasp logical reasoning.\n",
    "To ensure the safety of the model, the team employed various data cleaning and filtering techniques, including rigorous filtering for CSAM (child sexual abuse material), sensitive data filtering, and filtering based on content quality in compliance with Google’s policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI scientist cross the road?\n",
      "\n",
      "To get to the other algorithm side.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOllama(base_url=\"http://192.168.178.84:11434\", model=\"gemma:7b\")\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "topic = {\"topic\": \"Artificial Intelligence\"}\n",
    "\n",
    "print(chain.invoke(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = ChatOllama(model=\"gemma:7b\", base_url = \"http://192.168.178.84:11434\", callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"Why is the sky blue?\")\n",
    "]\n",
    "llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is the car red.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOllama(model=\"gemma:7b\", base_url = \"http://192.168.178.84:11434\")\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Why is the {topic} {color}.\"\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(topic=\"car\", color=\"red\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The sky appears blue due to a phenomenon called **Rayleigh Scattering**.\n",
       "\n",
       "**Rayleigh Scattering:**\n",
       "\n",
       "* Sunlight consists of various colors of the rainbow, including blue, red, green, and yellow.\n",
       "* As sunlight enters the Earth's atmosphere, particles of air scatter the different colors of light in all directions.\n",
       "* Scattered light, including the blue component, is scattered more effectively in the direction of the observer's eyes.\n",
       "\n",
       "**Scattering Intensity:**\n",
       "\n",
       "* The intensity of scattered light increases with the wavelength of light (shorter wavelengths like blue light scatter more effectively).\n",
       "* The scattered light is scattered in all directions, but the light scattered in the direction of the observer's eyes is more noticeable.\n",
       "\n",
       "**Appearance of the Sky:**\n",
       "\n",
       "* The scattered blue light from various air particles accumulates in the atmosphere, making the sky appear blue.\n",
       "* The intensity of blue color increases as the sun's rays pass through a greater amount of air, such as at sunrise and sunset.\n",
       "\n",
       "**Other Factors:**\n",
       "\n",
       "* The time of day and angle of the sun can influence the intensity of the blue sky.\n",
       "* Clouds and pollution can sometimes reduce the scattering of blue light, making the sky appear white or gray.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "Rayleigh scattering of sunlight by particles in the atmosphere causes the sky to appear blue. The scattered light in the direction of the observer's eyes is predominantly blue, giving the sky its characteristic color."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "result = chain.run(topic=\"sky\", color=\"blue\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral7B\n",
    "\n",
    "[Mistral is a 7.3B](https://mistral.ai/news/announcing-mistral-7b/) parameter model, distributed with the Apache license. It is available in both instruct (instruction following) and text completion. The Mistral AI team has noted that Mistral 7B:\n",
    "\n",
    "* Outperforms Llama 2 13B on all benchmarks\n",
    "* Outperforms Llama 1 34B on many benchmarks\n",
    "* Approaches CodeLlama 7B performance on code, while remaining good at English tasks\n",
    "\n",
    "Mistral7B can easy deploy in the cloud (AWS/GCP/Azure) using [vLLM](https://docs.mistral.ai/self-deployment/skypilot/) or locally via Ollama. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-modal with LlaVA\n",
    "\n",
    "[LLaVA](https://llava-vl.github.io) is a multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4. LLaVA 1.6 supporting 672x672, 336x1344, 1344x336 resolutions, a better visual reasoning and OCR capability with an improved visual instruction tuning data mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML, display\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_base64(pil_image):\n",
    "    \"\"\"\n",
    "    Convert PIL images to Base64 encoded strings\n",
    "\n",
    "    :param pil_image: PIL image\n",
    "    :return: Re-sized Base64 string\n",
    "    \"\"\"\n",
    "\n",
    "    buffered = BytesIO()\n",
    "    pil_image.save(buffered, format=\"JPEG\")  # You can change the format if needed\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return img_str\n",
    "\n",
    "def prompt_func(data):\n",
    "    text = data[\"text\"]\n",
    "    image = data[\"image\"]\n",
    "\n",
    "    image_part = {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{image}\",\n",
    "    }\n",
    "\n",
    "    content_parts = []\n",
    "\n",
    "    text_part = {\"type\": \"text\", \"text\": text}\n",
    "\n",
    "    content_parts.append(image_part)\n",
    "    content_parts.append(text_part)\n",
    "\n",
    "    return [HumanMessage(content=content_parts)]\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"\n",
    "    Disply base64 encoded string as image\n",
    "\n",
    "    :param img_base64:  Base64 string\n",
    "    \"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The image shows a building with classical architectural features, such as columns and a pediment. Above the entrance, there is a sign that reads \"Teatro alla Scala,\" which translates to \"Theater at La Scala.\" This suggests that the building is a theater, specifically La Scala, which is a famous opera house in Milan, Italy. The facade of the building has Italian flags on either side, indicating a connection to Italy or an event related to Italian culture. In front of the building, there are people standing and walking around, suggesting that this might be a public area where performances take place or where visitors gather before entering the theater. The sky is clear, and it appears to be late afternoon or early evening, as indicated by the soft lighting. \n",
      "Time elapsed (hh:mm:ss.ms) 0:34:22.660857\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../assets/teatro-alla-scala.jpeg\"\n",
    "pil_image = Image.open(file_path)\n",
    "\n",
    "image_b64 = convert_to_base64(pil_image)\n",
    "plt_img_base64(image_b64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(base_url=\"http://192.168.178.84:11434\", model=\"llava:v1.6\", temperature=0)\n",
    "\n",
    "chain = prompt_func | llm | StrOutputParser()\n",
    "\n",
    "start_time = datetime.now()\n",
    "query_chain = chain.invoke(\n",
    "    {\"text\": \"What is on this image?\", \"image\": image_b64}\n",
    ")\n",
    "\n",
    "print(query_chain)\n",
    "\n",
    "time_elapsed = datetime.now() - start_time\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking token usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Expression Language (LCEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
