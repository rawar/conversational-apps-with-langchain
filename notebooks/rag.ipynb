{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG - Retrieval-Augmented Generation\n",
    "\n",
    "RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.\n",
    "\n",
    "## How RAG works\n",
    "\n",
    "* Create external data\n",
    "* Retrieve relevant information\n",
    "* Augment the LLM prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from langchain_community.document_loaders import TextLoader, WebBaseLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain_community.vectorstores import SQLiteVSS\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from IPython.display import display, Markdown, JSON\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure embeddings model and the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"http://localhost:11434\"\n",
    "LLM_MODEL = \"gemma:7b\"\n",
    "EMBEDDINGS_MODEL = \"nomic-embed-text:latest\"\n",
    "llm = ChatOllama(base_url=HOST, model=LLM_MODEL, temperature=0)\n",
    "embeddings_model = OllamaEmbeddings(base_url=HOST, model=EMBEDDINGS_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-4\",\n",
    ")\n",
    "embeddings_model = AzureOpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the content from website https://en.wikipedia.org/wiki/GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/GPT-4\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(data)\n",
    "texts = [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Print/export\\n\\t\\n\\n\\nDownload as PDFPrintable version\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFrom Wikipedia, the free encyclopedia\\n\\n\\n2023 text-generating language model\\n\\n\\nGenerative Pre-trained Transformer 4 (GPT-4)Developer(s)OpenAIInitial releaseMarch\\xa014, 2023; 12 months ago\\xa0(2023-03-14)PredecessorGPT-3.5Type\\nMultimodal\\nLarge language model\\nGenerative pre-trained transformer\\nFoundation model\\nLicenseProprietaryWebsiteopenai.com/gpt-4\\xa0\\nPart of a series onMachine learningand data mining\\nParadigms\\nSupervised learning\\nUnsupervised learning\\nOnline learning\\nBatch learning\\nMeta-learning\\nSemi-supervised learning\\nSelf-supervised learning\\nReinforcement learning\\nCurriculum learning\\nRule-based learning\\nQuantum machine learning'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = FAISS.from_documents(\n",
    "    docs, \n",
    "    embeddings_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.save_local(\"faiss_rag_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "print(vector_db.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"How many parameters has GPT-4?\"\n",
    "docs_and_scores = vector_db.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Document(page_content='Sam Altman stated that the cost of training GPT-4 was more than $100 million.[45] News website Semafor claimed that they had spoken with \"eight people familiar with the inside story\" and found that GPT-4 had 1 trillion parameters.[46]', metadata={'source': 'https://en.wikipedia.org/wiki/GPT-4', 'title': 'GPT-4 - Wikipedia', 'language': 'en'}), 0.27346978)\n",
      "(Document(page_content='Background[edit]\\nFurther information: GPT-3 ยง\\xa0Background, and GPT-2 ยง\\xa0Background\\nOpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training.\"[8] It was based on the transformer architecture and trained on a large corpus of books.[9] The next year, they introduced GPT-2, a larger model that could generate coherent text.[10] In 2020, they introduced GPT-3, a model with 100 times as many parameters as GPT-2, that could perform various tasks with few examples.[11] GPT-3 was further improved into GPT-3.5, which was used to create the chatbot product ChatGPT.\\nRumors claim that GPT-4 has 1.76 trillion parameters, which was first estimated by the speed it was running and by George Hotz.[12]', metadata={'source': 'https://en.wikipedia.org/wiki/GPT-4', 'title': 'GPT-4 - Wikipedia', 'language': 'en'}), 0.28684634)\n",
      "(Document(page_content=\"Limitations[edit]\\nLike its predecessors, GPT-4 has been known to hallucinate, meaning that the outputs may include information not in the training data or that contradicts the user's prompt.[40]\\nGPT-4 also lacks transparency in its decision-making processes. If requested, the model is able to provide an explanation as to how and why it makes its decisions but these explanations are formed post-hoc; it's impossible to verify if those explanations truly reflect the actual process. In many cases, when asked to explain its logic, GPT-4 will give explanations that directly contradict its previous statements.[20]\", metadata={'source': 'https://en.wikipedia.org/wiki/GPT-4', 'title': 'GPT-4 - Wikipedia', 'language': 'en'}), 0.29306433)\n",
      "(Document(page_content='Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions.[4] GPT-4, equipped with vision capabilities (GPT-4V),[5] is capable of taking images as input on ChatGPT.[6] OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.[7]', metadata={'source': 'https://en.wikipedia.org/wiki/GPT-4', 'title': 'GPT-4 - Wikipedia', 'language': 'en'}), 0.30163202)\n"
     ]
    }
   ],
   "source": [
    "for doc in docs_and_scores:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever()\n",
    "\n",
    "# Version 1\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "# Version 2\n",
    "docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Sam Altman stated that the cost of training GPT-4 was more than $100 million.[45] News website Semafor claimed that they had spoken with \"eight people familiar with the inside story\" and found that GPT-4 had 1 trillion parameters.[46]', metadata={'source': 'https://en.wikipedia.org/wiki/GPT-4', 'title': 'GPT-4 - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='Background[edit]\\nFurther information: GPT-3 ยง\\xa0Background, and GPT-2 ยง\\xa0Background\\nOpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training.\"[8] It was based on the transformer architecture and trained on a large corpus of books.[9] The next year, they introduced GPT-2, a larger model that could generate coherent text.[10] In 2020, they introduced GPT-3, a model with 100 times as many parameters as GPT-2, that could perform various tasks with few examples.[11] GPT-3 was further improved into GPT-3.5, which was used to create the chatbot product ChatGPT.\\nRumors claim that GPT-4 has 1.76 trillion parameters, which was first estimated by the speed it was running and by George Hotz.[12]', metadata={'source': 'https://en.wikipedia.org/wiki/GPT-4', 'title': 'GPT-4 - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content=\"Limitations[edit]\\nLike its predecessors, GPT-4 has been known to hallucinate, meaning that the outputs may include information not in the training data or that contradicts the user's prompt.[40]\\nGPT-4 also lacks transparency in its decision-making processes. If requested, the model is able to provide an explanation as to how and why it makes its decisions but these explanations are formed post-hoc; it's impossible to verify if those explanations truly reflect the actual process. In many cases, when asked to explain its logic, GPT-4 will give explanations that directly contradict its previous statements.[20]\", metadata={'source': 'https://en.wikipedia.org/wiki/GPT-4', 'title': 'GPT-4 - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions.[4] GPT-4, equipped with vision capabilities (GPT-4V),[5] is capable of taking images as input on ChatGPT.[6] OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.[7]', metadata={'source': 'https://en.wikipedia.org/wiki/GPT-4', 'title': 'GPT-4 - Wikipedia', 'language': 'en'})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Answer the question based only on the supplied context. If you don't know the answer, say you don't know the answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Your answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "GPT-4 has 1 trillion parameters according to some sources, while rumors claim it has 1.76 trillion parameters. The precise number has not been officially confirmed by OpenAI."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = rag_chain.invoke(\"How many parameters has GPT-4\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the book 20.000 Leagues under the see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.gutenberg.org/cache/epub/164/pg164.txt'\n",
    "filename = '../data/twenty-thousand-leagues-under-the-sea.txt'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(filename)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "texts = [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding vectors \n",
    "\n",
    "We put all vectors into the sqlite-vss in a table named ```twenty_thousand_leagues_under_the_sea```.\n",
    "The db_file parameter is the name of the file you want as your sqlite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = SQLiteVSS.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embeddings_model,\n",
    "    table=\"twenty_thousand_leagues_under_the_sea\",\n",
    "    db_file=\"../vss.db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the Nautilus?\"\n",
    "docs = vector_db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is Ned's last name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Answer the question based only on the supplied context. If you don't know the answer, say you don't know the answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Your answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(base_url=HOST, model=MODEL, temperature=0)\n",
    "#set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"context\": retriever, \"question\": \"What is Professor Aronnax's name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
