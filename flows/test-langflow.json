{"id":"1f39cae6-753d-4934-b2ef-eac6b3bd2189","data":{"nodes":[{"id":"BaseChatModel-fK07l","type":"genericNode","position":{"x":1100.44,"y":265.63800109863286},"data":{"type":"BaseChatModel","node":{"template":{"metadata":{"type":"Dict[str, Any]","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"metadata","display_name":"Metadata","advanced":true,"dynamic":false,"info":"Metadata to add to the run trace.","title_case":true},"stop":{"type":"list","required":false,"placeholder":"","list":true,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"stop","display_name":"Stop Tokens","advanced":true,"dynamic":false,"info":"List of tokens to signal the model to stop generating text.","title_case":true},"tags":{"type":"list","required":false,"placeholder":"","list":true,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"tags","display_name":"Tags","advanced":true,"dynamic":false,"info":"Tags to add to the run trace.","title_case":true},"base_url":{"type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"base_url","display_name":"Base URL","advanced":false,"dynamic":false,"info":"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.","title_case":true,"value":"http://localhost:11434"},"cache":{"type":"bool","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"value":false,"fileTypes":[],"file_path":"","password":false,"name":"cache","display_name":"Cache","advanced":true,"dynamic":false,"info":"Enable or disable caching.","title_case":true},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Any, Dict, List, Optional\n\n# from langchain_community.chat_models import ChatOllama\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain_core.language_models.chat_models import BaseChatModel\n\n# from langchain.chat_models import ChatOllama\nfrom langflow import CustomComponent\n\n# whe When a callback component is added to Langflow, the comment must be uncommented.\n# from langchain.callbacks.manager import CallbackManager\n\n\nclass ChatOllamaComponent(CustomComponent):\n    display_name = \"ChatOllama\"\n    description = \"Local LLM for chat with Ollama.\"\n\n    def build_config(self) -> dict:\n        return {\n            \"base_url\": {\n                \"display_name\": \"Base URL\",\n                \"info\": \"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            },\n            \"model\": {\n                \"display_name\": \"Model Name\",\n                \"value\": \"llama2\",\n                \"info\": \"Refer to https://ollama.ai/library for more models.\",\n            },\n            \"temperature\": {\n                \"display_name\": \"Temperature\",\n                \"field_type\": \"float\",\n                \"value\": 0.8,\n                \"info\": \"Controls the creativity of model responses.\",\n            },\n            \"cache\": {\n                \"display_name\": \"Cache\",\n                \"field_type\": \"bool\",\n                \"info\": \"Enable or disable caching.\",\n                \"advanced\": True,\n                \"value\": False,\n            },\n            ### When a callback component is added to Langflow, the comment must be uncommented. ###\n            # \"callback_manager\": {\n            #     \"display_name\": \"Callback Manager\",\n            #     \"info\": \"Optional callback manager for additional functionality.\",\n            #     \"advanced\": True,\n            # },\n            # \"callbacks\": {\n            #     \"display_name\": \"Callbacks\",\n            #     \"info\": \"Callbacks to execute during model runtime.\",\n            #     \"advanced\": True,\n            # },\n            ########################################################################################\n            \"format\": {\n                \"display_name\": \"Format\",\n                \"field_type\": \"str\",\n                \"info\": \"Specify the format of the output (e.g., json).\",\n                \"advanced\": True,\n            },\n            \"metadata\": {\n                \"display_name\": \"Metadata\",\n                \"info\": \"Metadata to add to the run trace.\",\n                \"advanced\": True,\n            },\n            \"mirostat\": {\n                \"display_name\": \"Mirostat\",\n                \"options\": [\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n                \"info\": \"Enable/disable Mirostat sampling for controlling perplexity.\",\n                \"value\": \"Disabled\",\n                \"advanced\": True,\n            },\n            \"mirostat_eta\": {\n                \"display_name\": \"Mirostat Eta\",\n                \"field_type\": \"float\",\n                \"info\": \"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n                \"advanced\": True,\n            },\n            \"mirostat_tau\": {\n                \"display_name\": \"Mirostat Tau\",\n                \"field_type\": \"float\",\n                \"info\": \"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n                \"advanced\": True,\n            },\n            \"num_ctx\": {\n                \"display_name\": \"Context Window Size\",\n                \"field_type\": \"int\",\n                \"info\": \"Size of the context window for generating tokens. (Default: 2048)\",\n                \"advanced\": True,\n            },\n            \"num_gpu\": {\n                \"display_name\": \"Number of GPUs\",\n                \"field_type\": \"int\",\n                \"info\": \"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n                \"advanced\": True,\n            },\n            \"num_thread\": {\n                \"display_name\": \"Number of Threads\",\n                \"field_type\": \"int\",\n                \"info\": \"Number of threads to use during computation. (Default: detected for optimal performance)\",\n                \"advanced\": True,\n            },\n            \"repeat_last_n\": {\n                \"display_name\": \"Repeat Last N\",\n                \"field_type\": \"int\",\n                \"info\": \"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n                \"advanced\": True,\n            },\n            \"repeat_penalty\": {\n                \"display_name\": \"Repeat Penalty\",\n                \"field_type\": \"float\",\n                \"info\": \"Penalty for repetitions in generated text. (Default: 1.1)\",\n                \"advanced\": True,\n            },\n            \"tfs_z\": {\n                \"display_name\": \"TFS Z\",\n                \"field_type\": \"float\",\n                \"info\": \"Tail free sampling value. (Default: 1)\",\n                \"advanced\": True,\n            },\n            \"timeout\": {\n                \"display_name\": \"Timeout\",\n                \"field_type\": \"int\",\n                \"info\": \"Timeout for the request stream.\",\n                \"advanced\": True,\n            },\n            \"top_k\": {\n                \"display_name\": \"Top K\",\n                \"field_type\": \"int\",\n                \"info\": \"Limits token selection to top K. (Default: 40)\",\n                \"advanced\": True,\n            },\n            \"top_p\": {\n                \"display_name\": \"Top P\",\n                \"field_type\": \"float\",\n                \"info\": \"Works together with top-k. (Default: 0.9)\",\n                \"advanced\": True,\n            },\n            \"verbose\": {\n                \"display_name\": \"Verbose\",\n                \"field_type\": \"bool\",\n                \"info\": \"Whether to print out response text.\",\n            },\n            \"tags\": {\n                \"display_name\": \"Tags\",\n                \"field_type\": \"list\",\n                \"info\": \"Tags to add to the run trace.\",\n                \"advanced\": True,\n            },\n            \"stop\": {\n                \"display_name\": \"Stop Tokens\",\n                \"field_type\": \"list\",\n                \"info\": \"List of tokens to signal the model to stop generating text.\",\n                \"advanced\": True,\n            },\n            \"system\": {\n                \"display_name\": \"System\",\n                \"field_type\": \"str\",\n                \"info\": \"System to use for generating text.\",\n                \"advanced\": True,\n            },\n            \"template\": {\n                \"display_name\": \"Template\",\n                \"field_type\": \"str\",\n                \"info\": \"Template to use for generating text.\",\n                \"advanced\": True,\n            },\n        }\n\n    def build(\n        self,\n        base_url: Optional[str],\n        model: str,\n        mirostat: Optional[str],\n        mirostat_eta: Optional[float] = None,\n        mirostat_tau: Optional[float] = None,\n        ### When a callback component is added to Langflow, the comment must be uncommented.###\n        # callback_manager: Optional[CallbackManager] = None,\n        # callbacks: Optional[List[Callbacks]] = None,\n        #######################################################################################\n        repeat_last_n: Optional[int] = None,\n        verbose: Optional[bool] = None,\n        cache: Optional[bool] = None,\n        num_ctx: Optional[int] = None,\n        num_gpu: Optional[int] = None,\n        format: Optional[str] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        num_thread: Optional[int] = None,\n        repeat_penalty: Optional[float] = None,\n        stop: Optional[List[str]] = None,\n        system: Optional[str] = None,\n        tags: Optional[List[str]] = None,\n        temperature: Optional[float] = None,\n        template: Optional[str] = None,\n        tfs_z: Optional[float] = None,\n        timeout: Optional[int] = None,\n        top_k: Optional[int] = None,\n        top_p: Optional[int] = None,\n    ) -> BaseChatModel:\n        if not base_url:\n            base_url = \"http://localhost:11434\"\n\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(mirostat, 0)  # type: ignore\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": base_url,\n            \"cache\": cache,\n            \"model\": model,\n            \"mirostat\": mirostat_value,\n            \"format\": format,\n            \"metadata\": metadata,\n            \"tags\": tags,\n            ## When a callback component is added to Langflow, the comment must be uncommented.##\n            # \"callback_manager\": callback_manager,\n            # \"callbacks\": callbacks,\n            #####################################################################################\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": num_ctx,\n            \"num_gpu\": num_gpu,\n            \"num_thread\": num_thread,\n            \"repeat_last_n\": repeat_last_n,\n            \"repeat_penalty\": repeat_penalty,\n            \"temperature\": temperature,\n            \"stop\": stop,\n            \"system\": system,\n            \"template\": template,\n            \"tfs_z\": tfs_z,\n            \"timeout\": timeout,\n            \"top_k\": top_k,\n            \"top_p\": top_p,\n            \"verbose\": verbose,\n        }\n\n        # None Value remove\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not initialize Ollama LLM.\") from e\n\n        return output  # type: ignore\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":false,"dynamic":true,"info":"","title_case":true},"format":{"type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"format","display_name":"Format","advanced":true,"dynamic":false,"info":"Specify the format of the output (e.g., json).","title_case":true},"mirostat":{"type":"str","required":false,"placeholder":"","list":true,"show":true,"multiline":false,"value":"Disabled","fileTypes":[],"file_path":"","password":false,"options":["Disabled","Mirostat","Mirostat 2.0"],"name":"mirostat","display_name":"Mirostat","advanced":true,"dynamic":false,"info":"Enable/disable Mirostat sampling for controlling perplexity.","title_case":true},"mirostat_eta":{"type":"float","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"mirostat_eta","display_name":"Mirostat Eta","advanced":true,"dynamic":false,"info":"Learning rate for Mirostat algorithm. (Default: 0.1)","rangeSpec":{"min":-1,"max":1,"step":0.1},"title_case":true},"mirostat_tau":{"type":"float","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"mirostat_tau","display_name":"Mirostat Tau","advanced":true,"dynamic":false,"info":"Controls the balance between coherence and diversity of the output. (Default: 5.0)","rangeSpec":{"min":-1,"max":1,"step":0.1},"title_case":true},"model":{"type":"str","required":true,"placeholder":"","list":false,"show":true,"multiline":false,"value":"gemma:7b","fileTypes":[],"file_path":"","password":false,"name":"model","display_name":"Model Name","advanced":false,"dynamic":false,"info":"Refer to https://ollama.ai/library for more models.","title_case":true},"num_ctx":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"num_ctx","display_name":"Context Window Size","advanced":true,"dynamic":false,"info":"Size of the context window for generating tokens. (Default: 2048)","title_case":true},"num_gpu":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"num_gpu","display_name":"Number of GPUs","advanced":true,"dynamic":false,"info":"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)","title_case":true},"num_thread":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"num_thread","display_name":"Number of Threads","advanced":true,"dynamic":false,"info":"Number of threads to use during computation. (Default: detected for optimal performance)","title_case":true},"repeat_last_n":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"repeat_last_n","display_name":"Repeat Last N","advanced":true,"dynamic":false,"info":"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)","title_case":true},"repeat_penalty":{"type":"float","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"repeat_penalty","display_name":"Repeat Penalty","advanced":true,"dynamic":false,"info":"Penalty for repetitions in generated text. (Default: 1.1)","rangeSpec":{"min":-1,"max":1,"step":0.1},"title_case":true},"system":{"type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"system","display_name":"System","advanced":true,"dynamic":false,"info":"System to use for generating text.","title_case":true},"temperature":{"type":"float","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"value":"0.1","fileTypes":[],"file_path":"","password":false,"name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"Controls the creativity of model responses.","rangeSpec":{"min":-1,"max":1,"step":0.1},"title_case":true},"template":{"type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"template","display_name":"Template","advanced":true,"dynamic":false,"info":"Template to use for generating text.","title_case":true},"tfs_z":{"type":"float","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"tfs_z","display_name":"TFS Z","advanced":true,"dynamic":false,"info":"Tail free sampling value. (Default: 1)","rangeSpec":{"min":-1,"max":1,"step":0.1},"title_case":true},"timeout":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"timeout","display_name":"Timeout","advanced":true,"dynamic":false,"info":"Timeout for the request stream.","title_case":true},"top_k":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"top_k","display_name":"Top K","advanced":true,"dynamic":false,"info":"Limits token selection to top K. (Default: 40)","title_case":true},"top_p":{"type":"float","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"top_p","display_name":"Top P","advanced":true,"dynamic":false,"info":"Works together with top-k. (Default: 0.9)","rangeSpec":{"min":-1,"max":1,"step":0.1},"title_case":true},"verbose":{"type":"bool","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"value":true,"fileTypes":[],"file_path":"","password":false,"name":"verbose","display_name":"Verbose","advanced":false,"dynamic":false,"info":"Whether to print out response text.","title_case":true},"_type":"CustomComponent"},"description":"Local LLM for chat with Ollama.","base_classes":["BaseLanguageModel","BaseChatModel"],"display_name":"ChatOllama","documentation":"","custom_fields":{"base_url":null,"model":null,"mirostat":null,"mirostat_eta":null,"mirostat_tau":null,"repeat_last_n":null,"verbose":null,"cache":null,"num_ctx":null,"num_gpu":null,"format":null,"metadata":null,"num_thread":null,"repeat_penalty":null,"stop":null,"system":null,"tags":null,"temperature":null,"template":null,"tfs_z":null,"timeout":null,"top_k":null,"top_p":null},"output_types":["BaseChatModel"],"field_formatters":{},"beta":true},"id":"BaseChatModel-fK07l"},"selected":false,"width":384,"height":629,"positionAbsolute":{"x":1100.44,"y":265.63800109863286},"dragging":false},{"id":"LLMChain-IcwA8","type":"genericNode","position":{"x":2010.5200000000004,"y":544.998001098633},"data":{"type":"LLMChain","node":{"template":{"llm":{"type":"BaseLanguageModel","required":true,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"llm","display_name":"LLM","advanced":false,"dynamic":false,"info":"","title_case":true},"memory":{"type":"BaseMemory","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"memory","display_name":"Memory","advanced":false,"dynamic":false,"info":"","title_case":true},"prompt":{"type":"BasePromptTemplate","required":true,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"prompt","display_name":"Prompt","advanced":false,"dynamic":false,"info":"","title_case":true},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Callable, Optional, Union\n\nfrom langchain.chains import LLMChain\n\nfrom langflow import CustomComponent\nfrom langflow.field_typing import (\n    BaseLanguageModel,\n    BaseMemory,\n    BasePromptTemplate,\n    Chain,\n)\n\n\nclass LLMChainComponent(CustomComponent):\n    display_name = \"LLMChain\"\n    description = \"Chain to run queries against LLMs\"\n\n    def build_config(self):\n        return {\n            \"prompt\": {\"display_name\": \"Prompt\"},\n            \"llm\": {\"display_name\": \"LLM\"},\n            \"memory\": {\"display_name\": \"Memory\"},\n            \"code\": {\"show\": False},\n        }\n\n    def build(\n        self,\n        prompt: BasePromptTemplate,\n        llm: BaseLanguageModel,\n        memory: Optional[BaseMemory] = None,\n    ) -> Union[Chain, Callable, LLMChain]:\n        return LLMChain(prompt=prompt, llm=llm, memory=memory)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":false,"dynamic":true,"info":"","title_case":true},"_type":"CustomComponent"},"description":"Chain to run queries against LLMs","base_classes":["Chain","Callable","Chain","LLMChain"],"display_name":"LLMChain","documentation":"","custom_fields":{"prompt":null,"llm":null,"memory":null},"output_types":["Chain","Callable","LLMChain"],"field_formatters":{},"beta":true},"id":"LLMChain-IcwA8"},"selected":false,"width":384,"height":424,"positionAbsolute":{"x":2010.5200000000004,"y":544.998001098633},"dragging":false},{"id":"PromptTemplate-LDO22","type":"genericNode","position":{"x":1434.5200000000004,"y":1036.038001098633},"data":{"type":"PromptTemplate","node":{"template":{"output_parser":{"type":"BaseOutputParser","required":false,"placeholder":"","list":false,"show":false,"multiline":false,"fileTypes":[],"password":false,"name":"output_parser","advanced":false,"dynamic":true,"info":"","title_case":true},"input_types":{"type":"dict","required":false,"placeholder":"","list":false,"show":false,"multiline":false,"fileTypes":[],"password":false,"name":"input_types","advanced":false,"dynamic":true,"info":"","title_case":true},"input_variables":{"type":"str","required":true,"placeholder":"","list":true,"show":false,"multiline":false,"fileTypes":[],"password":false,"name":"input_variables","advanced":false,"dynamic":true,"info":"","title_case":true,"value":["comment_to_moderate"]},"metadata":{"type":"dict","required":false,"placeholder":"","list":false,"show":false,"multiline":false,"fileTypes":[],"password":false,"name":"metadata","advanced":false,"dynamic":true,"info":"","title_case":true},"name":{"type":"str","required":false,"placeholder":"","list":false,"show":false,"multiline":false,"fileTypes":[],"password":false,"name":"name","advanced":false,"dynamic":true,"info":"","title_case":true},"partial_variables":{"type":"dict","required":false,"placeholder":"","list":false,"show":false,"multiline":false,"fileTypes":[],"password":false,"name":"partial_variables","advanced":false,"dynamic":true,"info":"","title_case":true},"tags":{"type":"str","required":false,"placeholder":"","list":true,"show":false,"multiline":false,"fileTypes":[],"password":false,"name":"tags","advanced":false,"dynamic":true,"info":"","title_case":true},"template":{"type":"prompt","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"fileTypes":[],"password":false,"name":"template","advanced":false,"dynamic":true,"info":"","title_case":true,"value":"ou are the moderator of an online forum, you are strict and will not tolerate any negative comments.\nYou will receive a Original comment and if it is impolite you must transform in polite.\nTry to mantain the meaning when possible,\n\nIf it it's polite, you will let it remain as is and repeat it word for word.\nOriginal comment: {comment_to_moderate}"},"template_format":{"type":"str","required":false,"placeholder":"","list":false,"show":false,"multiline":false,"value":"f-string","fileTypes":[],"password":false,"name":"template_format","advanced":false,"dynamic":true,"info":"","title_case":true},"validate_template":{"type":"bool","required":false,"placeholder":"","list":false,"show":false,"multiline":false,"value":false,"fileTypes":[],"password":false,"name":"validate_template","advanced":false,"dynamic":true,"info":"","title_case":true},"_type":"PromptTemplate","comment_to_moderate":{"type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"comment_to_moderate","display_name":"comment_to_moderate","advanced":false,"input_types":["Document","BaseOutputParser"],"dynamic":false,"info":"","title_case":true}},"description":"A prompt template for a language model.","icon":null,"base_classes":["BasePromptTemplate","PromptTemplate","StringPromptTemplate"],"name":"","display_name":"PromptTemplate","documentation":"https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/","custom_fields":{"":["comment_to_moderate"]},"output_types":[],"full_path":null,"field_formatters":{},"beta":false,"error":null},"id":"PromptTemplate-LDO22","description":"A prompt template for a language model.","display_name":"PromptTemplate"},"selected":true,"width":384,"height":374,"dragging":false}],"edges":[{"source":"BaseChatModel-fK07l","sourceHandle":"{œbaseClassesœ:[œBaseLanguageModelœ,œBaseChatModelœ],œdataTypeœ:œBaseChatModelœ,œidœ:œBaseChatModel-fK07lœ}","target":"LLMChain-IcwA8","targetHandle":"{œfieldNameœ:œllmœ,œidœ:œLLMChain-IcwA8œ,œinputTypesœ:null,œtypeœ:œBaseLanguageModelœ}","data":{"targetHandle":{"fieldName":"llm","id":"LLMChain-IcwA8","inputTypes":null,"type":"BaseLanguageModel"},"sourceHandle":{"baseClasses":["BaseLanguageModel","BaseChatModel"],"dataType":"BaseChatModel","id":"BaseChatModel-fK07l"}},"style":{"stroke":"#555"},"className":"stroke-foreground  stroke-connection","animated":false,"id":"reactflow__edge-BaseChatModel-fK07l{œbaseClassesœ:[œBaseLanguageModelœ,œBaseChatModelœ],œdataTypeœ:œBaseChatModelœ,œidœ:œBaseChatModel-fK07lœ}-LLMChain-IcwA8{œfieldNameœ:œllmœ,œidœ:œLLMChain-IcwA8œ,œinputTypesœ:null,œtypeœ:œBaseLanguageModelœ}"},{"source":"PromptTemplate-LDO22","sourceHandle":"{œbaseClassesœ:[œBasePromptTemplateœ,œPromptTemplateœ,œStringPromptTemplateœ],œdataTypeœ:œPromptTemplateœ,œidœ:œPromptTemplate-LDO22œ}","target":"LLMChain-IcwA8","targetHandle":"{œfieldNameœ:œpromptœ,œidœ:œLLMChain-IcwA8œ,œinputTypesœ:null,œtypeœ:œBasePromptTemplateœ}","data":{"targetHandle":{"fieldName":"prompt","id":"LLMChain-IcwA8","inputTypes":null,"type":"BasePromptTemplate"},"sourceHandle":{"baseClasses":["BasePromptTemplate","PromptTemplate","StringPromptTemplate"],"dataType":"PromptTemplate","id":"PromptTemplate-LDO22"}},"style":{"stroke":"#555"},"className":"stroke-foreground  stroke-connection","animated":false,"id":"reactflow__edge-PromptTemplate-LDO22{œbaseClassesœ:[œBasePromptTemplateœ,œPromptTemplateœ,œStringPromptTemplateœ],œdataTypeœ:œPromptTemplateœ,œidœ:œPromptTemplate-LDO22œ}-LLMChain-IcwA8{œfieldNameœ:œpromptœ,œidœ:œLLMChain-IcwA8œ,œinputTypesœ:null,œtypeœ:œBasePromptTemplateœ}"}],"viewport":{"x":-16.194444444444798,"y":2.291666666666515,"zoom":0.6944444444444445}},"description":"Design, Develop, Dialogize.","name":"test-flow","last_tested_version":"0.6.15","is_component":false}